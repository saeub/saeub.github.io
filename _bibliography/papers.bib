@inproceedings{Carrer2024,
  title     = {Towards Holistic Human Evaluation of Automatic Text Simplification},
  author    = {Carrer, Luisa and Säuberli, Andreas and Kappus, Martin and Ebling, Sarah},
  booktitle = {Proceedings of the 4th Workshop on Human Evaluation of NLP Systems (HumEval)},
  year      = {2024},
  address   = {Turin, Italy},
  publisher = {European Language Resources Association},
  abstract  = {Text simplification refers to the process of rewording within a single language, moving from a standard form into an easy-to-understand one. Easy Language and Plain Language are two examples of simplified varieties aimed at improving readability and understanding for a wide-ranging audience. Human evaluation of automatic text simplification (ATS) is usually done by employing experts or crowdworkers to rate the generated texts. However, this approach does not include the target readers of simplified texts and does not reflect actual comprehensibility. In this paper, we explore different ways of measuring the quality of automatically simplified texts. We conducted a multi-faceted evaluation study involving end users, post-editors, and Easy Language experts, and applied a variety of qualitative and quantitative methods. We found differences in the perception and actual comprehension of the texts by different user groups. In addition, qualitative surveys and behavioral observations proved to be essential in interpreting the results. Finally, we discuss the advantages of comprehensive evaluations of ATS and provide recommendations for future work.}
}

@inproceedings{Saeuberli2024b,
  title     = {Automatic Generation and Evaluation of Reading Comprehension Test Items with Large Language Models},
  author    = {Säuberli, Andreas and Clematide, Simon},
  booktitle = {Proceedings of the 3rd Workshop on Tools and Resources to Empower People with REAding DIfficulties (READI)},
  year      = {2024},
  address   = {Turin, Italy},
  publisher = {European Language Resources Association},
  abstract  = {Reading comprehension tests are used in a variety of applications, reaching from education to assessing the comprehensibility of simplified texts. However, creating such tests manually and ensuring their quality is difficult and time-consuming. In this paper, we explore how large language models (LLMs) can be used to generate and evaluate multiple-choice reading comprehension items. To this end, we compiled a dataset of German reading comprehension items and developed a new protocol for human and automatic evaluation, including a metric we call \emph{text informativity}, which is based on guessability and answerability. We then used this protocol and the dataset to evaluate the quality of items generated by Llama 2 and GPT-4. Our results suggest that both models are capable of generating items of acceptable quality in a zero-shot setting, but GPT-4 clearly outperforms Llama 2. We also show that LLMs can be used for automatic evaluation by eliciting item reponses from them. In this scenario, evaluation results with GPT-4 were the most similar to human annotators. Overall, zero-shot generation with LLMs is a promising approach for generating and evaluating reading comprehension test items, in particular for languages without large amounts of available data.},
  pdf       = {https://arxiv.org/pdf/2404.07720.pdf},
  code      = {https://github.com/saeub/item-evaluation},
  data      = {https://github.com/saeub/dwlg},
  selected  = {true}
}

@inproceedings{Saeuberli2024a,
  author    = {Säuberli, Andreas and Holzknecht, Franz and Haller, Patrick and Deilen, Silvana and Schiffl, Laura and Hansen-Schirra, Silvia and Ebling, Sarah},
  title     = {Digital Comprehensibility Assessment of Simplified Texts among Persons with Intellectual Disabilities},
  year      = {2024},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
  location  = {Honolulu, HI, USA},
  series    = {CHI '24},
  abstract  = {Text simplification refers to the process of increasing the comprehensibility of texts. Automatic text simplification models are most commonly evaluated by experts or crowdworkers instead of the primary target groups of simplified texts, such as persons with intellectual disabilities. We conducted an evaluation study of text comprehensibility including participants with and without intellectual disabilities reading unsimplified, automatically and manually simplified German texts on a tablet computer. We explored four different approaches to measuring comprehensibility: multiple-choice comprehension questions, perceived difficulty ratings, response time, and reading speed. The results revealed significant variations in these measurements, depending on the reader group and whether the text had undergone automatic or manual simplification. For the target group of persons with intellectual disabilities, comprehension questions emerged as the most reliable measure, while analyzing reading speed provided valuable insights into participants' reading behavior.},
  pdf       = {https://arxiv.org/pdf/2402.13094.pdf},
  selected  = {true}
}

@article{Saeuberli2023,
  author   = {Säuberli, Andreas and Hansen-Schirra, Silvia and Holzknecht, Franz and Gutermuth, Silke and Deilen, Silvana and Schiffl, Laura and Ebling, Sarah},
  title    = {Enabling text comprehensibility assessment for people with intellectual disabilities using a mobile application},
  journal  = {Frontiers in Communication},
  volume   = {8},
  year     = {2023},
  url      = {https://www.frontiersin.org/articles/10.3389/fcomm.2023.1175625},
  doi      = {10.3389/fcomm.2023.1175625},
  abstract = {In research on Easy Language and automatic text simplification, it is imperative to evaluate the comprehensibility of texts by presenting them to target users and assessing their level of comprehension. Target readers often include people with intellectual or other disabilities, which renders conducting experiments more challenging and time-consuming. In this paper, we introduce Okra, an openly available touchscreen-based application to facilitate the inclusion of people with disabilities in studies of text comprehensibility. It implements several tasks related to reading comprehension and cognition and its user interface is optimized toward the needs of people with intellectual disabilities (IDs). We used Okra in a study with 16 participants with IDs and tested for effects of modality, comparing reading comprehension results when texts are read on paper and on an iPad. We found no evidence of such an effect on multiple-choice comprehension questions and perceived difficulty ratings, but reading time was significantly longer on paper. We also tested the feasibility of assessing cognitive skill levels of participants in Okra, and discuss problems and possible improvements. We will continue development of the application and use it for evaluating automatic text simplification systems in the future.},
  pdf      = {https://www.frontiersin.org/articles/10.3389/fcomm.2023.1175625/pdf},
  code     = {https://github.com/saeub/okra},
  selected = {true}
}

@inproceedings{Haller2022,
  title     = {Eye-tracking based classification of {M}andarin {C}hinese readers with and without dyslexia using neural sequence models},
  author    = {Haller, Patrick  and Säuberli, Andreas  and Kiener, Sarah  and Pan, Jinger  and Yan, Ming  and Jäger, Lena},
  editor    = {Štajner, Sanja  and Saggion, Horacio  and Ferrés, Daniel  and Shardlow, Matthew  and Sheang, Kim Cheng  and North, Kai  and Zampieri, Marcos  and Xu, Wei},
  booktitle = {Proceedings of the Workshop on Text Simplification, Accessibility, and Readability (TSAR-2022)},
  year      = {2022},
  address   = {Abu Dhabi, United Arab Emirates (Virtual)},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.tsar-1.10},
  doi       = {10.18653/v1/2022.tsar-1.10},
  pages     = {111--118},
  abstract  = {Eye movements are known to reflect cognitive processes in reading, and psychological reading research has shown that eye gaze patterns differ between readers with and without dyslexia. In recent years, researchers have attempted to classify readers with dyslexia based on their eye movements using Support Vector Machines (SVMs). However, these approaches (i) are based on highly aggregated features averaged over all words read by a participant, thus disregarding the sequential nature of the eye movements, and (ii) do not consider the linguistic stimulus and its interaction with the reader{'}s eye movements. In the present work, we propose two simple sequence models that process eye movements on the entire stimulus without the need of aggregating features across the sentence. Additionally, we incorporate the linguistic stimulus into the model in two ways{---}contextualized word embeddings and manually extracted linguistic features. The models are evaluated on a Mandarin Chinese dataset containing eye movements from children with and without dyslexia. Our results show that (i) even for a logographic script such as Chinese, sequence models are able to classify dyslexia on eye gaze sequences, reaching state-of-the-art performance, and (ii) incorporating the linguistic stimulus does not help to improve classification performance.},
  pdf       = {https://aclanthology.org/2022.tsar-1.10.pdf},
  code      = {https://github.com/hallerp/dyslexia-seqmod}
}

@article{Saeuberli2022,
  title    = {{L}au{SA}n at e{R}isk 2022: Simply and Effectively Optimizing Text Classification for Early Detection},
  author   = {Säuberli, Andreas and Cho, Sooyeon and Stahlhut, Laura},
  journal  = {Working Notes of CLEF},
  pages    = {5--8},
  year     = {2022},
  abstract = {The goal of early detection tasks at eRisk is to classify social media users as early as possible, based on streams of posts written by those users. We present two simple strategies of adapting standard text classification models in order to optimize them for early detection: concatenating the posts in different ways during training and inference, and continuously moving the decision boundary at inference time. We applied these approaches to two different text classification architectures based on pre-trained language models in eRisk 2022's Task 2 (early detection of depression), and were able to reach top 5 placements in all time-sensitive evaluation metrics. A systematic post-submission ablation study confirmed that both strategies were effective at optimizing for early detection.},
  pdf      = {https://ceur-ws.org/Vol-3180/paper-79.pdf}
}

@article{Ebling2022,
  author   = {Ebling, Sarah and Battisti, Alessia and Kostrzewa, Marek and Pfütze, Dominik and Rios, Annette and Säuberli, Andreas and Spring, Nicolas},
  title    = {Automatic Text Simplification for {G}erman},
  journal  = {Frontiers in Communication},
  volume   = {7},
  year     = {2022},
  url      = {https://www.frontiersin.org/articles/10.3389/fcomm.2022.706718},
  doi      = {10.3389/fcomm.2022.706718},
  abstract = {The article at hand aggregates the work of our group in automatic processing of simplified German. We present four parallel (standard/simplified German) corpora compiled and curated by our group. We report on the creation of a gold standard of sentence alignments from the four sources for evaluating automatic alignment methods on this gold standard. We show that one of the alignment methods performs best on the majority of the data sources. We used two of our corpora as a basis for the first sentence-based neural machine translation (NMT) approach toward automatic simplification of German. In follow-up work, we extended our model to render it capable of explicitly operating on multiple levels of simplified German. We show that using source-side language level labels improves performance with regard to two evaluation metrics commonly applied to measuring the quality of automatic text simplification.},
  pdf      = {https://www.frontiersin.org/articles/10.3389/fcomm.2022.706718/pdf}
}

@inproceedings{Saeuberli2021,
  author    = {Säuberli, Andreas},
  title     = {Measuring Text Comprehension for People with Reading Difficulties Using a Mobile Application},
  year      = {2021},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://dl.acm.org/doi/10.1145/3441852.3476474},
  doi       = {10.1145/3441852.3476474},
  booktitle = {Proceedings of the 23rd International ACM SIGACCESS Conference on Computers and Accessibility},
  location  = {Virtual Event, USA},
  series    = {ASSETS '21},
  abstract  = {Measuring text comprehension is crucial for evaluating the accessibility of texts in Easy Language. However, accurate and objective comprehension tests tend to be expensive, time-consuming and sometimes difficult to implement for target groups of Easy Language. In this paper, we propose using computer-based testing with touchscreen devices as a means to simplify and accelerate data collection using comprehension tests, and to facilitate experiments with less proficient readers. We demonstrate this by designing and implementing a mobile touchscreen application and validating its effectiveness in an experiment with people with intellectual disabilities. The results suggest that there is no difference in terms of task difficulty between measuring comprehension using the mobile application and a traditional paper-and-pencil test. Moreover, reading times appear to be faster in the application than on paper.}
}

@inproceedings{Rios2021,
  title     = {A New Dataset and Efficient Baselines for Document-level Text Simplification in {G}erman},
  author    = {Rios, Annette and Spring, Nicolas and Kew, Tannon and Kostrzewa, Marek and Säuberli, Andreas and Müller, Mathias and Ebling, Sarah},
  booktitle = {Proceedings of the Third Workshop on New Frontiers in Summarization},
  year      = {2021},
  address   = {Online and in Dominican Republic},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.newsum-1.16},
  doi       = {10.18653/v1/2021.newsum-1.16},
  pages     = {152--161},
  abstract  = {The task of document-level text simplification is very similar to summarization with the additional difficulty of reducing complexity. We introduce a newly collected data set of German texts, collected from the Swiss news magazine 20 Minuten ({`}20 Minutes{'}) that consists of full articles paired with simplified summaries. Furthermore, we present experiments on automatic text simplification with the pretrained multilingual mBART and a modified version thereof that is more memory-friendly, using both our new data set and existing simplification corpora. Our modifications of mBART let us train at a lower memory cost without much loss in performance, in fact, the smaller mBART even improves over the standard model in a setting with multiple simplification levels.},
  pdf       = {https://aclanthology.org/2021.newsum-1.16.pdf},
  code      = {https://github.com/a-rios/longmbart}
}

@inproceedings{Saeuberli2020,
  title     = {Benchmarking Data-driven Automatic Text Simplification for {G}erman},
  author    = {Säuberli, Andreas and Ebling, Sarah and Volk, Martin},
  booktitle = {Proceedings of the 1st Workshop on Tools and Resources to Empower People with REAding DIfficulties (READI)},
  year      = {2020},
  address   = {Marseille, France},
  publisher = {European Language Resources Association},
  pages     = {41--48},
  selected  = {true},
  abstract  = {Automatic text simplification is an active research area, and there are first systems for English, Spanish, Portuguese, and Italian. For {G}erman, no data-driven approach exists to this date, due to a lack of training data. In this paper, we present a parallel corpus of news items in German with corresponding simplifications on two complexity levels. The simplifications have been produced according to a well-documented set of guidelines. We then report on experiments in automatically simplifying the German news items using state-of-the-art neural machine translation techniques. We demonstrate that despite our small parallel corpus, our neural models were able to learn essential features of simplified language, such as lexical substitutions, deletion of less relevant words and phrases, and sentence shortening.},
  pdf       = {https://aclanthology.org/2020.readi-1.7.pdf}
}

@inproceedings{Battisti2020,
  title     = {A Corpus for Automatic Readability Assessment and Text Simplification of {G}erman},
  author    = {Battisti, Alessia and Pfütze, Dominik and Säuberli, Andreas and Kostrzewa, Marek and Ebling, Sarah},
  booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},
  year      = {2020},
  address   = {Marseille, France},
  publisher = {European Language Resources Association},
  pages     = {3302--3311},
  abstract  = {In this paper, we present a corpus for use in automatic readability assessment and automatic text simplification for German, the first of its kind for this language. The corpus is compiled from web sources and consists of parallel as well as monolingual-only (simplified German) data amounting to approximately 6,200 documents (nearly 211,000 sentences). As a unique feature, the corpus contains information on text structure (e.g., paragraphs, lines), typography (e.g., font type, font style), and images (content, position, and dimensions). While the importance of considering such information in machine learning tasks involving simplified language, such as readability assessment, has repeatedly been stressed in the literature, we provide empirical evidence for its benefit. We also demonstrate the added value of leveraging monolingual-only data for automatic text simplification via machine translation through applying back-translation, a data augmentation technique.},
  pdf       = {https://aclanthology.org/2020.lrec-1.404.pdf}
}
